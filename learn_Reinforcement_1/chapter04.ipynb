{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "书中使用的应该是`Tensorflow 1.x`系列，我安装的版本是`Tensorflow1.15.0`，`cpu版本`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我的测试代码\n",
    "\n",
    "自己使用 `Tensorflow1.15.0` 时的一些测试验证代码，毕竟不知道书中用的是什么版本，这个真的得吐槽一下，写书难道不说明一下自己使用的python，tensorflow版本吗？这不是coder的常识吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow版本号  :  1.15.0\n",
      "Tensorflow安装路径:  ['d:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf1\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\api\\\\_v1', 'd:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf1\\\\lib\\\\site-packages\\\\tensorflow_estimator\\\\python\\\\estimator\\\\api\\\\_v1', 'd:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf1\\\\lib\\\\site-packages\\\\tensorflow_core', 'd:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf1\\\\lib\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v1']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('Tensorflow版本号  : ',tf.__version__)\n",
    "print('Tensorflow安装路径: ',tf.__path__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 的基本使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data_x = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "data_y = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "# 这里将用到 placeholder 这个结构 。 它会定义一些占位变量， \n",
    "# 这些变量在定义计算图时并不会被明确指定，它的数值将在执行计算图时才被赋值，\n",
    "# 所以这个结构一般被用于传入网络的数据。 我们可以从上面的输入数据中看到数据的维度为 2 \n",
    "x = tf.placeholder(tf.float32, shape=[None, 2], name='input')\n",
    "# 其中 shape= [None， 2］表示在定义占位符时只知道输入的第二维是 2 ，第一维的大小并不确定\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w1 = tf.Variable(tf.random_uniform([2,4], -1, 1), name='w1')\n",
    "b1 = tf.Variable(tf.zeros(4, dtype=np.float32), name='b1')\n",
    "\n",
    "\n",
    "w2 = tf.get_variable('w2', shape=[4,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable('b2', initializer=tf.constant(0.0))\n",
    "\n",
    "\n",
    "z1 = tf.sigmoid(tf.matmul(x, w1) + b1)  # 第1层输出\n",
    "z2 = tf.sigmoid(tf.matmul(z1, w2) + b2)  # 第2层输出\n",
    "\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name='output')  # 定义一个输出值\n",
    "loss = tf.nn.l2_loss(z2 - y )  # 比较输出值和目标值的差距\n",
    "\n",
    "# 完成目标函数的设定后，接下来定义优化器和优化策略\n",
    "opt = tf.train.GradientDescentOptimizer(0.05)  # 定义了一个最基本的梯度下降法，它的学习率是 0.05\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x shap :  (4, 2)\n",
      "data_y shap :  (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print('data_x shap : ', data_x.shape)\n",
    "print('data_y shap : ', data_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:loss=0.5318878889083862\n",
      "10000:loss=0.020341254770755768\n",
      "20000:loss=0.004817883018404245\n",
      "30000:loss=0.0025588541757315397\n",
      "40000:loss=0.0017095129005610943\n",
      "50000:loss=0.0012724973494186997\n",
      "60000:loss=0.0010085290996357799\n",
      "70000:loss=0.0008327396353706717\n",
      "80000:loss=0.0007075909525156021\n",
      "90000:loss=0.0006142731290310621\n",
      "\n",
      "graph.get_operations():\n",
      "-----------------------\n",
      "[<tf.Operation 'input' type=Placeholder>, <tf.Operation 'random_uniform/shape' type=Const>, <tf.Operation 'random_uniform/min' type=Const>, <tf.Operation 'random_uniform/max' type=Const>, <tf.Operation 'random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'random_uniform/sub' type=Sub>, <tf.Operation 'random_uniform/mul' type=Mul>, <tf.Operation 'random_uniform' type=Add>, <tf.Operation 'w1' type=VariableV2>, <tf.Operation 'w1/Assign' type=Assign>, <tf.Operation 'w1/read' type=Identity>, <tf.Operation 'zeros/shape_as_tensor' type=Const>, <tf.Operation 'zeros/Const' type=Const>, <tf.Operation 'zeros' type=Fill>, <tf.Operation 'b1' type=VariableV2>, <tf.Operation 'b1/Assign' type=Assign>, <tf.Operation 'b1/read' type=Identity>, <tf.Operation 'w2/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'w2/Initializer/random_uniform/min' type=Const>, <tf.Operation 'w2/Initializer/random_uniform/max' type=Const>, <tf.Operation 'w2/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'w2/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'w2/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'w2/Initializer/random_uniform' type=Add>, <tf.Operation 'w2' type=VariableV2>, <tf.Operation 'w2/Assign' type=Assign>, <tf.Operation 'w2/read' type=Identity>, <tf.Operation 'Const' type=Const>, <tf.Operation 'b2' type=VariableV2>, <tf.Operation 'b2/Assign' type=Assign>, <tf.Operation 'b2/read' type=Identity>, <tf.Operation 'MatMul' type=MatMul>, <tf.Operation 'add' type=AddV2>, <tf.Operation 'Sigmoid' type=Sigmoid>, <tf.Operation 'MatMul_1' type=MatMul>, <tf.Operation 'add_1' type=AddV2>, <tf.Operation 'Sigmoid_1' type=Sigmoid>, <tf.Operation 'output' type=Placeholder>, <tf.Operation 'sub' type=Sub>, <tf.Operation 'L2Loss' type=L2Loss>, <tf.Operation 'gradients/Shape' type=Const>, <tf.Operation 'gradients/grad_ys_0' type=Const>, <tf.Operation 'gradients/Fill' type=Fill>, <tf.Operation 'gradients/L2Loss_grad/mul' type=Mul>, <tf.Operation 'gradients/sub_grad/Shape' type=Shape>, <tf.Operation 'gradients/sub_grad/Shape_1' type=Shape>, <tf.Operation 'gradients/sub_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'gradients/sub_grad/Sum' type=Sum>, <tf.Operation 'gradients/sub_grad/Reshape' type=Reshape>, <tf.Operation 'gradients/sub_grad/Neg' type=Neg>, <tf.Operation 'gradients/sub_grad/Sum_1' type=Sum>, <tf.Operation 'gradients/sub_grad/Reshape_1' type=Reshape>, <tf.Operation 'gradients/sub_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'gradients/Sigmoid_1_grad/SigmoidGrad' type=SigmoidGrad>, <tf.Operation 'gradients/add_1_grad/Shape' type=Shape>, <tf.Operation 'gradients/add_1_grad/Shape_1' type=Shape>, <tf.Operation 'gradients/add_1_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'gradients/add_1_grad/Sum' type=Sum>, <tf.Operation 'gradients/add_1_grad/Reshape' type=Reshape>, <tf.Operation 'gradients/add_1_grad/Sum_1' type=Sum>, <tf.Operation 'gradients/add_1_grad/Reshape_1' type=Reshape>, <tf.Operation 'gradients/add_1_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/add_1_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/add_1_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'gradients/MatMul_1_grad/MatMul' type=MatMul>, <tf.Operation 'gradients/MatMul_1_grad/MatMul_1' type=MatMul>, <tf.Operation 'gradients/MatMul_1_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/MatMul_1_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/MatMul_1_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'gradients/Sigmoid_grad/SigmoidGrad' type=SigmoidGrad>, <tf.Operation 'gradients/add_grad/Shape' type=Shape>, <tf.Operation 'gradients/add_grad/Shape_1' type=Shape>, <tf.Operation 'gradients/add_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>, <tf.Operation 'gradients/add_grad/Sum' type=Sum>, <tf.Operation 'gradients/add_grad/Reshape' type=Reshape>, <tf.Operation 'gradients/add_grad/Sum_1' type=Sum>, <tf.Operation 'gradients/add_grad/Reshape_1' type=Reshape>, <tf.Operation 'gradients/add_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/add_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/add_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'gradients/MatMul_grad/MatMul' type=MatMul>, <tf.Operation 'gradients/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'gradients/MatMul_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/MatMul_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'GradientDescent/learning_rate' type=Const>, <tf.Operation 'GradientDescent/update_w1/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'GradientDescent/update_b1/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'GradientDescent/update_w2/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'GradientDescent/update_b2/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'GradientDescent' type=NoOp>, <tf.Operation 'init' type=NoOp>]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for iter in range(100000):\n",
    "        loss_val, _ = sess.run([loss, train_op], feed_dict={x:data_x, y:data_y})\n",
    "        if iter % 10000 == 0:\n",
    "            print('{}:loss={}'.format(iter, loss_val))\n",
    "    graph = tf.get_default_graph()\n",
    "    print('\\ngraph.get_operations():\\n-----------------------')\n",
    "    li_operations = graph.get_operations()\n",
    "    print(li_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "1  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "2  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "3  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "4  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "5  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "6  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "7  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "8  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "9  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "10  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "11  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "12  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "13  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "14  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "15  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "16  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "17  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "18  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "19  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "20  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "21  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "22  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "23  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "24  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "25  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "26  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "27  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "28  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "29  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "30  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "31  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "32  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "33  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "34  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "35  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "36  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "37  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "38  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "39  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "40  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "41  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "42  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "43  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "44  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "45  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "46  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "47  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "48  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "49  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "50  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "51  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "52  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "53  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "54  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "55  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "56  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "57  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "58  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "59  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "60  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "61  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "62  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "63  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "64  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "65  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "66  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "67  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "68  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "69  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "70  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "71  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "72  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "73  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "74  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "75  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "76  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "77  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "78  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "79  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "80  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "81  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "82  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "83  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "84  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "85  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "86  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "87  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "88  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "89  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "90  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "91  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "92  :  <class 'tensorflow.python.framework.ops.Operation'>\n",
      "93  :  <class 'tensorflow.python.framework.ops.Operation'>\n"
     ]
    }
   ],
   "source": [
    "for i,op in enumerate(li_operations):\n",
    "    print(i, ' : ',type(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 原理介绍\n",
    "## 创建变量的 scope\n",
    "\n",
    "### test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置变量\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.name :   123/456/789/a:0\n",
      "b.name :   789/b:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.name_scope('123'):\n",
    "      with tf.name_scope('456'):  # 用的是name_scope\n",
    "        with tf.variable_scope('789'):  # 用的是variable_scope\n",
    "            a = tf.Variable(1,name='a')  # 对name_scope 和 variable_scope 都响应\n",
    "            print('a.name :  ',a.name)\n",
    "            b = tf.get_variable('b',1)  # 只对 variable_scope 响应\n",
    "            print('b.name :  ',b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test2\n",
    "\n",
    "name_scope 重复出现时，系统会自动改名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置变量\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.name :   123/456/789/a:0\n",
      "b.name :   789/b:0\n",
      "c.name :   123/456_1/789/c:0\n",
      "d.name :   123/456_1/789/d:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('123'):\n",
    "    with tf.name_scope('456'):\n",
    "        with tf.variable_scope('789'):\n",
    "            a = tf.Variable(1,name='a')\n",
    "            print('a.name :  ',a.name)\n",
    "            b = tf.get_variable('b',1)\n",
    "            print('b.name :  ',b.name)\n",
    "    with tf.name_scope('456'):  # name_scope 重复出现时，系统会自动改名\n",
    "        with tf.variable_scope('789'):\n",
    "            c = tf.Variable(1,name='c')\n",
    "            print('c.name :  ',c.name)\n",
    "            d = tf.Variable(1,name='d')\n",
    "            print('d.name :  ',d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test3\n",
    "\n",
    "同一个 scope 下出现两遍 variable_scope 时， scope 的名字不会出现名称修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置变量\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d.name :   123/456/789/d:0\n",
      "e.name :   789/e:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('123'):\n",
    "    with tf.name_scope('456'):\n",
    "        with tf.variable_scope('789'):\n",
    "            d = tf.Variable(1,name='d')\n",
    "            print('d.name :  ',d.name)\n",
    "        with tf.variable_scope('789'):\n",
    "            e = tf.get_variable('e',1)\n",
    "            print('e.name :  ',e.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name_scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置变量\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.name :   f:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('123'):\n",
    "    with tf.name_scope(None):\n",
    "        c = tf.Variable(1,name='f')\n",
    "        print('c.name :  ',c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable_scope\n",
    "\n",
    "实际上， variable_scope 和 name_scope 是两个有点相互独立的命名体系， \n",
    "\n",
    "- name_scope只对通过 Variable 创建的变量有效;\n",
    "- 而 variable_scope 还可以为 get_variable 这个可以复用的变量的方法服务;\n",
    "\n",
    "\n",
    "它首先调用了 name_scope 方法，然后在 name_scop巳的 context 中调用了另外一个生成 variable_scope 的方法。\n",
    "\n",
    "我们将整体过程分成五步：\n",
    "\n",
    "1. 获取default_varscope\n",
    "2. 获取var _store\n",
    "3. 组合新 scope 的名字\n",
    "4. 创建新的 varscope 并返回\n",
    "5. 回收 scope 信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相同点和不同点\n",
    "\n",
    "总体来看，两种 scope 存在一些相同和不同，让我们来总结一下 。\n",
    "1. 两种 scope 形成的命名空间将在 Variable 创建的变量上产生影响 。\n",
    "2. variable_scope 创建的命名空间将对 get_variable 的变量名产生影响 。\n",
    "3. 创建 named_scope 时，遇到同名的 scope ，系统会自动改名创建一个新的 scope。\n",
    "4. 创建 named_scope 时，可以通过将名字设置为 None 抹掉前面设定的所有命名空间的名字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个 Variable 背后的故事\n",
    "\n",
    "### tf.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'a/initial_value' type=Const>, <tf.Operation 'a' type=VariableV2>, <tf.Operation 'a/Assign' type=Assign>, <tf.Operation 'a/read' type=Identity>]\n",
      "------------\n",
      "\n",
      "__________op[0]:__________\n",
      "name: \"a/initial_value\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_INT32\n",
      "      tensor_shape {\n",
      "      }\n",
      "      int_val: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[1]:__________\n",
      "name: \"a\"\n",
      "op: \"VariableV2\"\n",
      "attr {\n",
      "  key: \"container\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shared_name\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[2]:__________\n",
      "name: \"a/Assign\"\n",
      "op: \"Assign\"\n",
      "input: \"a\"\n",
      "input: \"a/initial_value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"validate_shape\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[3]:__________\n",
      "name: \"a/read\"\n",
      "op: \"Identity\"\n",
      "input: \"a\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.Variable(1, name='a')\n",
    "g = tf.get_default_graph()\n",
    "ops = g.get_operations()\n",
    "print(ops)\n",
    "print('------------\\n')\n",
    "for i,op in enumerate(ops):\n",
    "    print('__________op[%d]:__________'%i)\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.get_variable()\n",
    "\n",
    "上面产生了4个op，这个会产生更多的op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'a/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'a/Initializer/random_uniform/min' type=Const>, <tf.Operation 'a/Initializer/random_uniform/max' type=Const>, <tf.Operation 'a/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'a/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'a/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'a/Initializer/random_uniform' type=Add>, <tf.Operation 'a' type=VariableV2>, <tf.Operation 'a/Assign' type=Assign>, <tf.Operation 'a/read' type=Identity>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.get_variable('a',1)\n",
    "g = tf.get_default_graph()\n",
    "ops = g.get_operations()\n",
    "print(ops)\n",
    "# print('------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________op[0]:__________\n",
      "name: \"a/Initializer/random_uniform/shape\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_INT32\n",
      "      tensor_shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      int_val: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[1]:__________\n",
      "name: \"a/Initializer/random_uniform/min\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: -1.7320507764816284\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[2]:__________\n",
      "name: \"a/Initializer/random_uniform/max\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: 1.7320507764816284\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[3]:__________\n",
      "name: \"a/Initializer/random_uniform/RandomUniform\"\n",
      "op: \"RandomUniform\"\n",
      "input: \"a/Initializer/random_uniform/shape\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"seed\"\n",
      "  value {\n",
      "    i: 0\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"seed2\"\n",
      "  value {\n",
      "    i: 0\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[4]:__________\n",
      "name: \"a/Initializer/random_uniform/sub\"\n",
      "op: \"Sub\"\n",
      "input: \"a/Initializer/random_uniform/max\"\n",
      "input: \"a/Initializer/random_uniform/min\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[5]:__________\n",
      "name: \"a/Initializer/random_uniform/mul\"\n",
      "op: \"Mul\"\n",
      "input: \"a/Initializer/random_uniform/RandomUniform\"\n",
      "input: \"a/Initializer/random_uniform/sub\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[6]:__________\n",
      "name: \"a/Initializer/random_uniform\"\n",
      "op: \"Add\"\n",
      "input: \"a/Initializer/random_uniform/mul\"\n",
      "input: \"a/Initializer/random_uniform/min\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[7]:__________\n",
      "name: \"a\"\n",
      "op: \"VariableV2\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"container\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shared_name\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[8]:__________\n",
      "name: \"a/Assign\"\n",
      "op: \"Assign\"\n",
      "input: \"a\"\n",
      "input: \"a/Initializer/random_uniform\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"validate_shape\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[9]:__________\n",
      "name: \"a/read\"\n",
      "op: \"Identity\"\n",
      "input: \"a\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@a\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[10]:__________\n",
      "name: \"b/Initializer/random_uniform/shape\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_INT32\n",
      "      tensor_shape {\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      int_val: 2\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[11]:__________\n",
      "name: \"b/Initializer/random_uniform/min\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: -1.2247449159622192\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[12]:__________\n",
      "name: \"b/Initializer/random_uniform/max\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: 1.2247449159622192\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[13]:__________\n",
      "name: \"b/Initializer/random_uniform/RandomUniform\"\n",
      "op: \"RandomUniform\"\n",
      "input: \"b/Initializer/random_uniform/shape\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"seed\"\n",
      "  value {\n",
      "    i: 0\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"seed2\"\n",
      "  value {\n",
      "    i: 0\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[14]:__________\n",
      "name: \"b/Initializer/random_uniform/sub\"\n",
      "op: \"Sub\"\n",
      "input: \"b/Initializer/random_uniform/max\"\n",
      "input: \"b/Initializer/random_uniform/min\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[15]:__________\n",
      "name: \"b/Initializer/random_uniform/mul\"\n",
      "op: \"Mul\"\n",
      "input: \"b/Initializer/random_uniform/RandomUniform\"\n",
      "input: \"b/Initializer/random_uniform/sub\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[16]:__________\n",
      "name: \"b/Initializer/random_uniform\"\n",
      "op: \"Add\"\n",
      "input: \"b/Initializer/random_uniform/mul\"\n",
      "input: \"b/Initializer/random_uniform/min\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[17]:__________\n",
      "name: \"b\"\n",
      "op: \"VariableV2\"\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"container\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: 2\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shared_name\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[18]:__________\n",
      "name: \"b/Assign\"\n",
      "op: \"Assign\"\n",
      "input: \"b\"\n",
      "input: \"b/Initializer/random_uniform\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"validate_shape\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[19]:__________\n",
      "name: \"b/read\"\n",
      "op: \"Identity\"\n",
      "input: \"b\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@b\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "__________op[20]:__________\n",
      "name: \"add\"\n",
      "op: \"AddV2\"\n",
      "input: \"a/read\"\n",
      "input: \"b/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,op in enumerate(ops):\n",
    "    print('__________op[%d]:__________'%i)\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算符\n",
    "原来语句使用`c = a + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Operation'>\n",
      "------------\n",
      "name: \"add\"\n",
      "op: \"AddV2\"\n",
      "input: \"a/read\"\n",
      "input: \"b/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n",
      "------------\n",
      "输入： \n",
      " [<tf.Tensor 'a/read:0' shape=(1,) dtype=float32>, <tf.Tensor 'b/read:0' shape=(2,) dtype=float32>]\n",
      "输出： \n",
      " [<tf.Tensor 'add:0' shape=(2,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.get_variable('a',1)\n",
    "b = tf.get_variable('b',2)\n",
    "c = a + b\n",
    "g = tf.get_default_graph()\n",
    "ops = g.get_operations()\n",
    "print (type(ops[-1]))\n",
    "print('------------')\n",
    "print (ops[-1])\n",
    "\n",
    "print('------------')\n",
    "print('输入： \\n',ops[-1].inputs[:])\n",
    "print('输出： \\n',ops[-1].outputs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "使用`c = tf.add(a, b, 'c')` 替换原来的 `c = a + b`\n",
    "\n",
    "对比输出，发现：\n",
    "\n",
    "我们发现代码和计算图存在一定的不同，为了编程的方便，一些繁杂的设定被系统以默认值替代，但有时会为我们带来麻烦，如果前面的加法出现错\n",
    "误（例如参数运算的 Tensor 维度不一致），那么 `add:O`  一定不如 `c:O` 容易定位，所以要尽可能地为每一个 `Op` 起好名字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Operation'>\n",
      "------------\n",
      "name: \"c\"\n",
      "op: \"Add\"\n",
      "input: \"a/read\"\n",
      "input: \"b/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n",
      "------------\n",
      "输入： \n",
      " [<tf.Tensor 'a/read:0' shape=(1,) dtype=float32>, <tf.Tensor 'b/read:0' shape=(2,) dtype=float32>]\n",
      "输出： \n",
      " [<tf.Tensor 'c:0' shape=(2,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.get_variable('a',1)\n",
    "b = tf.get_variable('b',2)\n",
    "c = tf.add(a, b, 'c')  # 替换原来的 c = a + b\n",
    "g = tf.get_default_graph()\n",
    "ops = g.get_operations()\n",
    "print (type(ops[-1]))\n",
    "print('------------')\n",
    "print (ops[-1])\n",
    "\n",
    "print('------------')\n",
    "print('输入： \\n',ops[-1].inputs[:])\n",
    "print('输出： \\n',ops[-1].outputs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf. gradients-不懂--------\n",
    "\n",
    "讲反向计算的一些原理特性和源码分析-没搞懂2020.1.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'Const' type=Const>, <tf.Operation 'Const_1' type=Const>, <tf.Operation 'add' type=AddV2>]\n",
      "------------\n",
      "[<tf.Operation 'Const' type=Const>, <tf.Operation 'Const_1' type=Const>, <tf.Operation 'add' type=AddV2>, <tf.Operation 'gradients/Shape' type=Const>, <tf.Operation 'gradients/grad_ys_0' type=Const>, <tf.Operation 'gradients/Fill' type=Fill>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(1.0)\n",
    "c = a + b\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "ops1 = g.get_operations()\n",
    "print (ops1)\n",
    "\n",
    "print('------------')\n",
    "\n",
    "grad = tf.gradients(c, [a,b])\n",
    "ops2 = g.get_operations()\n",
    "print (ops2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存路径 :  X:\\0_temp\\0_jys\\0_我的学习研究\\0_机器学习-强化学习\\0_learn_pythonLib\\learn_Reinforcement_1\\logs\n"
     ]
    }
   ],
   "source": [
    "sPath = %pwd\n",
    "sPath = sPath + r\"\\logs\"\n",
    "print('保存路径 : ',sPath)\n",
    "\n",
    "tf.summary.FileWriter(sPath, g).close()  # windows保存要用绝对路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后在cmd窗口下键入下面的代码，利用TensorBoard 显示绘制的信息：\n",
    "```\n",
    "tensorboard --logdir=sPath\n",
    "```\n",
    "其中`sPath`就是保存的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer-不懂--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'a/initial_value' type=Const>, <tf.Operation 'a' type=VariableV2>, <tf.Operation 'a/Assign' type=Assign>, <tf.Operation 'a/read' type=Identity>, <tf.Operation 'sub/y' type=Const>, <tf.Operation 'sub' type=Sub>]\n",
      "------------\n",
      "[<tf.Operation 'a/initial_value' type=Const>, <tf.Operation 'a' type=VariableV2>, <tf.Operation 'a/Assign' type=Assign>, <tf.Operation 'a/read' type=Identity>, <tf.Operation 'sub/y' type=Const>, <tf.Operation 'sub' type=Sub>, <tf.Operation 'gradients/Shape' type=Const>, <tf.Operation 'gradients/grad_ys_0' type=Const>, <tf.Operation 'gradients/Fill' type=Fill>, <tf.Operation 'gradients/sub_grad/Neg' type=Neg>, <tf.Operation 'gradients/sub_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency_1' type=Identity>]\n",
      "------------\n",
      "[<tf.Operation 'a/initial_value' type=Const>, <tf.Operation 'a' type=VariableV2>, <tf.Operation 'a/Assign' type=Assign>, <tf.Operation 'a/read' type=Identity>, <tf.Operation 'sub/y' type=Const>, <tf.Operation 'sub' type=Sub>, <tf.Operation 'gradients/Shape' type=Const>, <tf.Operation 'gradients/grad_ys_0' type=Const>, <tf.Operation 'gradients/Fill' type=Fill>, <tf.Operation 'gradients/sub_grad/Neg' type=Neg>, <tf.Operation 'gradients/sub_grad/tuple/group_deps' type=NoOp>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency' type=Identity>, <tf.Operation 'gradients/sub_grad/tuple/control_dependency_1' type=Identity>, <tf.Operation 'GradientDescent/learning_rate' type=Const>, <tf.Operation 'GradientDescent/update_a/ApplyGradientDescent' type=ApplyGradientDescent>, <tf.Operation 'GradientDescent' type=NoOp>]\n",
      "------------\n",
      "保存路径 :  X:\\0_temp\\0_jys\\0_我的学习研究\\0_机器学习-强化学习\\0_learn_pythonLib\\learn_Reinforcement_1\\logs\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # 重置变量\n",
    "\n",
    "#import tensorflow as tf\n",
    "a = tf.Variable(1.0 ,name='a')\n",
    "loss = a - 1\n",
    "opt = tf.train.GradientDescentOptimizer(0.1)\n",
    "g = tf.get_default_graph()\n",
    "print(g.get_operations())\n",
    "print('------------')\n",
    "\n",
    "grad = opt.compute_gradients(loss)\n",
    "print(g.get_operations())\n",
    "print('------------')\n",
    "\n",
    "train_op = opt.apply_gradients(grad)\n",
    "print(g.get_operations())\n",
    "print('------------')\n",
    "\n",
    "sPath = %pwd\n",
    "sPath = sPath + r\"\\logs\"\n",
    "print('保存路径 : ',sPath)\n",
    "tf.summary.FileWriter(sPath, g).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 的反向传播技巧-不懂--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong1():\n",
    "    # --------------------------\n",
    "    tf.reset_default_graph()  # 重置变量\n",
    "    # --------------------------\n",
    "    with tf.name_scope('normal'):\n",
    "        x = tf.placeholder(tf.float32, [],'x')\n",
    "        a = tf.Variable(0.0, 'a')\n",
    "        c = tf.random_uniform([], 0.0, 1.0)\n",
    "        op = a.assign(c)\n",
    "        with tf.control_dependencies([op]):\n",
    "            f = a * x\n",
    "            g = tf.gradients(f, x)\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        print (s.run([f,g], feed_dict={x:1}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong2():\n",
    "    # --------------------------\n",
    "    tf.reset_default_graph()  # 重置变量\n",
    "    # --------------------------\n",
    "    with tf.name_scope('normal'):\n",
    "        x = tf.placeholder(tf.float32, [],'a')\n",
    "        a = tf.Variable(0.0, 'w')\n",
    "        c = tf.random_uniform([], 0.0, 1.0)\n",
    "        op = a.assign(c)\n",
    "        c2 = tf.random_uniform([], 0.0, 1.0)\n",
    "        op2 = a.assign(c2)\n",
    "        with tf.control_dependencies([op]):\n",
    "            f = a * x\n",
    "            with tf.control_dependencies([op2]):\n",
    "                g = tf.gradients(f, x)\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        print( s.run([f, g], feed_dict={x: 1}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right1():\n",
    "    # --------------------------\n",
    "    tf.reset_default_graph()  # 重置变量\n",
    "    # --------------------------\n",
    "    with tf.name_scope('normal'):\n",
    "        x = tf.placeholder(tf.float32, [],'a')\n",
    "        a1 = tf.random_uniform([], 0.0, 1.0)\n",
    "        a2 = tf.random_uniform([], 0.0, 1.0)\n",
    "        f1 = a1 * x\n",
    "        f2 = a2 * x\n",
    "        f = f2 + tf.stop_gradient(f1 - f2)\n",
    "        g = tf.gradients(f, x)\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        print (s.run([f, g], feed_dict={x: 1}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right2():\n",
    "    # --------------------------\n",
    "    tf.reset_default_graph()  # 重置变量\n",
    "    # --------------------------\n",
    "    @tf.RegisterGradient(\"mult_grad\")\n",
    "    def _mult_grad(op, grad):\n",
    "        c2 = np.random.uniform(0.0, 1.0)\n",
    "        return  op.inputs[1] * grad, c2 * grad\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [])\n",
    "    a = tf.random_uniform([], 0.0, 1.0)\n",
    "    with g.gradient_override_map({\"Mul\":\"mult_grad\"}):\n",
    "        f = tf.multiply(a, x)\n",
    "        g = tf.gradients(f, x)\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        print (s.run([f, g], feed_dict={x: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1357274, [0.1357274]]\n"
     ]
    }
   ],
   "source": [
    "wrong1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25120497, [0.25120497]]\n"
     ]
    }
   ],
   "source": [
    "wrong2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62621486, [0.3169099]]\n"
     ]
    }
   ],
   "source": [
    "right1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6795342, [0.18168582]]\n"
     ]
    }
   ],
   "source": [
    "right2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arg_scope的使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "(0,)\n",
      "{'a': 1, 'b': None, 'c': [1]}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "tf.reset_default_graph()  # 重置变量\n",
    "# --------------------------\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "add_arg_scope = tf.contrib.framework.add_arg_scope\n",
    "arg_scope = tf.contrib.framework.arg_scope\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def func1(*args, **kwargs):\n",
    "    return (args, kwargs)\n",
    "\n",
    "with arg_scope((func1,), a=1, b=None, c=[1]):\n",
    "    args, kwargs = func1(0)\n",
    "    print(args)\n",
    "    print(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 的分布式训练\n",
    "\n",
    "## 基于 MPI 的数据并行模型\n",
    "\n",
    "1. Scatter\n",
    "2. Gather\n",
    "3. Allgather\n",
    "4. MPI_Bcast\n",
    "5. MPI_Reduce\n",
    "6. MPI_Allreduce\n",
    "\n",
    "## MPI 的实现： mpi_adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 TensorFlow 实现经典网络结构\n",
    "\n",
    "本节将展示基于 TensorFlow 的一些经典网络实现，这些网络包括:\n",
    "- 多层感知器（ Multi Layer Perceptron, MLP ）;\n",
    "- 卷积神经网络（ Convolutional Neural Network, CNN ）;\n",
    "- 循环神经网络（ Recurrent Neural Network ,RNN）。\n",
    "\n",
    "\n",
    "## 多层感知机\n",
    "\n",
    "多层感知器主要由全连接层和非线性激活层组成，全连接层一般由矩阵乘法表示，输入的向量 $z$ 和参数矩阵 $W$ 相乘后，得到的结果和向量 $b$ 相加，就得到了结果 $z$ ，计算过程如下所示：\n",
    "\n",
    "$$z=Wx+b$$\n",
    "\n",
    "由于输出中的每一个值要经过所有输入值的计算，因此这个结构被称为全连接层 。非线性激活层一般由线性整流函数（ Rectified Linear Unit, ReLU ）表示，它可以**将输入的负数部分变为 0 ，这样模型便满足了一定的非线性效果**。 对于一个 3 层网络组成的感知器。\n",
    "\n",
    "![IMG20200128092501.png](.\\img\\IMG20200128092501.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "tf.reset_default_graph()  # 重置变量\n",
    "# --------------------------\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "# 网络参数\n",
    "n_hidden_1 = 256 # 第1层网络神经元数量\n",
    "n_hidden_2 = 256 # 第2层网络神经元数量\n",
    "n_input = 784 # 数据输入维度\n",
    "n_classes = 10 # 数据输出维度\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# 模型中的参数定义\n",
    "weights = {\n",
    "    'h1' : tf.Variable( tf.random_normal([n_input,    n_hidden_1]) ),\n",
    "    'h2' : tf.Variable( tf.random_normal([n_hidden_1, n_hidden_2]) ),\n",
    "    'out': tf.Variable( tf.random_normal([n_hidden_2, n_classes] ) )\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x):\n",
    "    # 第一层全连接层和非线性层，下同\n",
    "    layer_1 = tf.nn.relu( tf.add(tf.matmul(x,       weights['h1']), biases['b1']) )\n",
    "    layer_2 = tf.nn.relu( tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) )\n",
    "    out     =                    tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "\n",
    "卷积神经网络的卷积层可 以在提取局部信息的同时尽可能地减少计算量 。VGG Net 是卷积神经网络曾经的代表作， 它在 网络结构设计上拥有自 己的特点，其中一些设计思路也被很多网络沿用至今。\n",
    "\n",
    "![IMG20200128092708.png](.\\img\\IMG20200128092708.png)\n",
    "\n",
    "整个模型分为 6 个部分，前 5 个部分由卷积层组成， 最后一个部分由全连接层组成。 \n",
    "\n",
    "在每一个卷积部分内， 图像的尺度并没有发生变化， 只有在进入下一个部分时，图像的长宽依次减半，但与此同时，它的通道数也扩大了一倍，这就相当于图像的信息在不断地集中变少， 而对图像分析的方式在不断增多 。 \n",
    "\n",
    "VGG Net的很多设计也被后来的模型使用 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络的世界还有很多优秀的模型结构 ， 例如**残差网络**等，希望读者能够自行学习了解 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "tf.reset_default_graph()  # 重置变量\n",
    "# --------------------------\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "# 5个卷积部分的输出通道数\n",
    "filter_depth = [64, 128, 256, 512] \n",
    "VGG16_NUM_HIDDEN_1, VGG16_NUM_HIDDEN_2 = 4096, 1000\n",
    "\n",
    "def submodel(x, iters, k_width, in_depth, out_depth):\n",
    "    for i in range(iters):\n",
    "        cur_in_depth = in_depth if i == 0 else out_depth\n",
    "        w = tf.Variable(tf.truncated_normal([k_width, k_width, cur_in_depth, out_depth], stddev=0.01))\n",
    "        b = tf.Variable(tf.zeros([out_depth]))\n",
    "        x = tf.nn.relu(tf.nn.conv2d(x, w, [1,1,1,1], padding='SAME') + b)\n",
    "    x = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    return x\n",
    "\n",
    "def fc(x, in_size, out_size):\n",
    "    w = tf.Variable(tf.truncated_normal([in_size, out_size]))\n",
    "    b = tf.Variable(tf.constant(0.0, shape=[out_size]))\n",
    "    out = tf.matmul(x, w) + b\n",
    "    return tf.nn.relu(out)\n",
    "  \n",
    "def vgg16(x, filter_depth, hidden1, hidden2, img_height, img_width, img_depth, num_labels):\n",
    "    out = x\n",
    "    k_w = 3\n",
    "    # 1st part\n",
    "    k_d1 = filter_depth[0]\n",
    "    out = submodel(out, 2, k_w, img_depth, k_d1)\n",
    "    # 2nd part\n",
    "    k_d2 = filter_depth[1]\n",
    "    out = submodel(out, 2, k_w, k_d1, k_d2)\n",
    "    # 3rd part\n",
    "    k_d3 = filter_depth[2]\n",
    "    out = submodel(out, 3, k_w, k_d2, k_d3)\n",
    "    # 4th part\n",
    "    k_d4 = filter_depth[3]\n",
    "    out = submodel(out, 3, k_w, k_d3, k_d4)\n",
    "    # 5th part\n",
    "    out = submodel(out, 3, k_w, k_d4, k_d4)\n",
    "\n",
    "    element_num = ((img_height // (2 ** 5)) * (img_width // (2 ** 5)) * k_d4)\n",
    "    out = tf.reshape(out, [-1, element_num])\n",
    "\n",
    "    # fully connected part\n",
    "    out = fc(out, element_num, VGG16_NUM_HIDDEN_1)\n",
    "    out = tf.nn.dropout(out, 0.5)\n",
    "    out = fc(out, VGG16_NUM_HIDDEN_1, VGG16_NUM_HIDDEN_2)\n",
    "    out = tf.nn.dropout(out, 0.5)\n",
    "\n",
    "    return fc(out, VGG16_NUM_HIDDEN_2, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "在前面介绍的模型中，我们可以通过传入某个固定的输入得到固定的输出 ，模型并不能对输入数据产生记忆 。 当我们遇到一些序列预测问题时＼这种基于时间或位置关系的问题又需要考虑时间或空间上的依赖关系，因此前面提到的模型不能满足这样的需求，于是我们就需要使用循环神经网络。 \n",
    "\n",
    "\n",
    "\n",
    "循环神经网络实现了对序列数据的建模和拟合，当我们使用模型计算出某一输入数据的结构后，部分信息就被记录到模型中，这些信息会在下一个输入计算时产生作用，这些记录的信息被称为隐含层的状态信息 。\n",
    "\n",
    "模型的整体结构如图 4-24 所示，其中 $x$ 表示模型的输入， $h$ 表示模型的隐含状态， 。$o$表示每一时刻的输出，而 $U 、 V$ 和 $w$表示模型的参数。 图 4-24 中左边展示的是聚合版本的模型，经过展开后可以得到右边的样子 ， 这样可以清晰地看出时刻之间的依赖关系 。\n",
    "\n",
    "\n",
    "![IMG20200128145840.png](.\\img\\IMG20200128145840.png)\n",
    "\n",
    "\n",
    "经典的 时仆J 模型在训练时存在梯度消失的问题 ，为了解决这个问题，前人发明了很多改进版的模型，而 LSTM ( Long Short Term Memory ）是其中最常见的模型，它能很好地解决因为时间跨度长而导致的梯度消失问题，它的基本结构如图 4-25 所示 。\n",
    "\n",
    "\n",
    "![IMG20200128150300.png](.\\img\\IMG20200128150300.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "tf.reset_default_graph()  # 重置变量\n",
    "# --------------------------\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "# 网络参数\n",
    "num_input = 28 # 数据输入的维度\n",
    "timesteps = 28 # 数据输入的总时长\n",
    "num_hidden = 128 # 隐含层的维度\n",
    "num_classes = 10 # 算法最终的输出维度\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# 定义LSTM之外的全连接层的参数\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "本章介绍了 TensorFlow 的基本使用方法和一些常见的与训练相关的内容，让我们回顾一下。\n",
    "\n",
    "1. TensorFlow 的使用主要包括：计算图定义、操作运算地点安排和计算图执行三部分。\n",
    "2. TensorFlow 内部的一些运行机理很值得读者做一定的研究 。\n",
    "3. 使用 MPI 的通信方式可以使 TensorFlow 完成分布式训练。\n",
    "4. TensorFlow 可以实现常见的各种经典模型 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
